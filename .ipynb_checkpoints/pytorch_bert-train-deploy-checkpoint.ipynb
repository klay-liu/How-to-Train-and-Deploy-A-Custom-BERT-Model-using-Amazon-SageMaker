{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2117fb-b50b-411e-8c3b-e52083be78b6",
   "metadata": {},
   "source": [
    "# Using Script Mode in SageMaker(BERT Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005e6c7-0b57-437f-a396-bc353b901495",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Enviroment in Sagemaker Studio:\n",
    "\n",
    "Instance type: `ml.t3.medium`\n",
    "\n",
    "Kernel: `Pytorch 1.4 Python 3.6 CPU Opitimized`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544fac6b-a9a0-4691-9ec0-bd9a45f32ad1",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b18104-450e-444e-89a7-6cdb4ed3675a",
   "metadata": {},
   "source": [
    "![Train and Deploy a Custom BERT Model in Sagemaker with Script Mode .png](https://d-ehlicnlmgqny.studio.us-east-1.sagemaker.aws/jupyter/default/files/tutorial/Train%20and%20Deploy%20a%20Custom%20BERT%20Model%20in%20Sagemaker%20with%20Script%20Mode%20.png?_xsrf=2%7Cf17ea0a5%7C59b349f0f19a4676c2f89ae3150c860f%7C1664774910)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0817ba24-a68f-4afc-a198-14135797f029",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Submission Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56592e68-e487-41ff-a81c-8046d53a6fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.19.25 requires botocore==1.20.25, but you have botocore 1.26.10 which is incompatible.\n",
      "awscli 1.19.25 requires s3transfer<0.4.0,>=0.3.0, but you have s3transfer 0.5.2 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade \"sagemaker>=2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecfd2025-2462-4455-9933-26f9d4eafa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.112.0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f86fdc84-cba0-4057-9af8-724ab4f82ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"epochs\": \"1\", \n",
    "                   \"batch_size\": \"32\", \n",
    "                   \"val_batch_size\": \"64\",\n",
    "                   \"max_len\": 200,\n",
    "                   \"label_col\": 'will_recommend',\n",
    "                   \"text_col\": 'review',\n",
    "                   \"n_classes\": 2\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d6d932-4460-4cc1-9778-0d97ec79a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point=\"scripts/train.py\",\n",
    "        base_job_name=\"sagemaker-script-mode-for-bert\",\n",
    "        name='hf-bert-model',\n",
    "        instance_type='ml.g4dn.12xlarge',\n",
    "        instance_count=1,\n",
    "        role=get_execution_role(),\n",
    "        transformers_version='4.6',\n",
    "        pytorch_version='1.7',\n",
    "        py_version='py36',\n",
    "        hyperparameters = hyperparameters,\n",
    "        code_location='s3://bucket/model-artifacts'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9162bb6c-b3c0-4f6a-ab48-edfa3df442ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = huggingface_estimator.sagemaker_session.upload_data('data/train.csv', bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b9c727-ab0d-45ab-a7c8-216e88b94779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-10 06:45:54 Starting - Starting the training job...\n",
      "2022-10-10 06:46:20 Starting - Preparing the instances for trainingProfilerReport-1665384354: InProgress\n",
      ".........\n",
      "2022-10-10 06:47:45 Downloading - Downloading input data\n",
      "2022-10-10 06:47:45 Training - Downloading the training image..................\n",
      "2022-10-10 06:50:39 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-10-10 06:50:39,343 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-10-10 06:50:39,390 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-10-10 06:50:39,397 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-10-10 06:50:39,973 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": \"32\",\n",
      "        \"epochs\": \"1\",\n",
      "        \"label_col\": \"will_recommend\",\n",
      "        \"max_len\": 200,\n",
      "        \"n_classes\": 2,\n",
      "        \"text_col\": \"review\",\n",
      "        \"val_batch_size\": \"64\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-script-mode-for-bert-2022-10-10-06-45-54-223\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-tutorial-demos/model-artifacts/sagemaker-script-mode-for-bert-2022-10-10-06-45-54-223/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sagemaker_entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sagemaker_entry_point.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":\"32\",\"epochs\":\"1\",\"label_col\":\"will_recommend\",\"max_len\":200,\"n_classes\":2,\"text_col\":\"review\",\"val_batch_size\":\"64\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sagemaker_entry_point.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sagemaker_entry_point\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-tutorial-demos/model-artifacts/sagemaker-script-mode-for-bert-2022-10-10-06-45-54-223/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":\"32\",\"epochs\":\"1\",\"label_col\":\"will_recommend\",\"max_len\":200,\"n_classes\":2,\"text_col\":\"review\",\"val_batch_size\":\"64\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-script-mode-for-bert-2022-10-10-06-45-54-223\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-tutorial-demos/model-artifacts/sagemaker-script-mode-for-bert-2022-10-10-06-45-54-223/source/sourcedir.tar.gz\",\"module_name\":\"sagemaker_entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sagemaker_entry_point.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"32\",\"--epochs\",\"1\",\"--label_col\",\"will_recommend\",\"--max_len\",\"200\",\"--n_classes\",\"2\",\"--text_col\",\"review\",\"--val_batch_size\",\"64\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_LABEL_COL=will_recommend\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_LEN=200\u001b[0m\n",
      "\u001b[34mSM_HP_N_CLASSES=2\u001b[0m\n",
      "\u001b[34mSM_HP_TEXT_COL=review\u001b[0m\n",
      "\u001b[34mSM_HP_VAL_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 sagemaker_entry_point.py --batch_size 32 --epochs 1 --label_col will_recommend --max_len 200 --n_classes 2 --text_col review --val_batch_size 64\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.004 algo-1:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.107 algo-1:26 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.107 algo-1:26 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.107 algo-1:26 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.108 algo-1:26 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.108 algo-1:26 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.370 algo-1:26 INFO hook.py:591] name:bert.embeddings.word_embeddings.weight count_params:22268928\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.370 algo-1:26 INFO hook.py:591] name:bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.370 algo-1:26 INFO hook.py:591] name:bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.370 algo-1:26 INFO hook.py:591] name:bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.370 algo-1:26 INFO hook.py:591] name:bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.370 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.370 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.370 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.370 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.371 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.372 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.373 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.374 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.375 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.376 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:out.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.377 algo-1:26 INFO hook.py:591] name:out.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.378 algo-1:26 INFO hook.py:593] Total Trainable Params: 108311810\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.378 algo-1:26 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-10-10 06:50:54.378 algo-1:26 INFO hook.py:488] Hook is writing from the hook with pid: 26\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [0/16909 (0%)]#011Loss: 0.497937\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3200/16909 (38%)]#011Loss: 0.000680\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/16909 (75%)]#011Loss: 0.000199\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mEpochs: 1 | Train Loss:  0.000 | Train Accuracy:  0.995 | Val Loss:  0.000 | Val Accuracy:  1.000\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 213k/213k [00:00<00:00, 68.1MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 48.6kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 436k/436k [00:00<00:00, 52.2MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 570/570 [00:00<00:00, 1.01MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]#015Downloading:   2%|▏         | 7.84M/436M [00:00<00:05, 78.4MB/s]#015Downloading:   4%|▎         | 15.7M/436M [00:00<00:05, 78.6MB/s]#015Downloading:   5%|▌         | 23.7M/436M [00:00<00:05, 78.8MB/s]#015Downloading:   7%|▋         | 31.6M/436M [00:00<00:05, 79.0MB/s]#015Downloading:   9%|▉         | 39.6M/436M [00:00<00:05, 79.1MB/s]#015Downloading:  11%|█         | 47.5M/436M [00:00<00:04, 79.3MB/s]#015Downloading:  13%|█▎        | 55.5M/436M [00:00<00:04, 79.5MB/s]#015Downloading:  15%|█▍        | 63.5M/436M [00:00<00:04, 79.5MB/s]#015Downloading:  16%|█▋        | 71.5M/436M [00:00<00:04, 79.7MB/s]#015Downloading:  18%|█▊        | 79.5M/436M [00:01<00:04, 79.8MB/s]#015Downloading:  20%|██        | 87.5M/436M [00:01<00:04, 79.8MB/s]#015Downloading:  22%|██▏       | 95.5M/436M [00:01<00:04, 79.8MB/s]#015Downloading:  24%|██▎       | 103M/436M [00:01<00:04, 79.8MB/s] #015Downloading:  26%|██▌       | 111M/436M [00:01<00:04, 79.7MB/s]#015Downloading:  27%|██▋       | 119M/436M [00:01<00:03, 79.7MB/s]#015Downloading:  29%|██▉       | 127M/436M [00:01<00:03, 79.7MB/s]#015Downloading:  31%|███       | 135M/436M [00:01<00:03, 79.7MB/s]#015Downloading:  33%|███▎      | 143M/436M [00:01<00:03, 79.7MB/s]#015Downloading:  35%|███▍      | 151M/436M [00:01<00:03, 79.7MB/s]#015Downloading:  37%|███▋      | 159M/436M [00:02<00:03, 79.5MB/s]#015Downloading:  38%|███▊      | 167M/436M [00:02<00:03, 79.3MB/s]#015Downloading:  40%|████      | 175M/436M [00:02<00:03, 79.4MB/s]#015Downloading:  42%|████▏     | 183M/436M [00:02<00:03, 79.4MB/s]#015Downloading:  44%|████▍     | 191M/436M [00:02<00:03, 79.5MB/s]#015Downloading:  46%|████▌     | 199M/436M [00:02<00:02, 79.6MB/s]#015Downloading:  47%|████▋     | 207M/436M [00:02<00:02, 79.6MB/s]#015Downloading:  49%|████▉     | 215M/436M [00:02<00:02, 79.6MB/s]#015Downloading:  51%|█████     | 223M/436M [00:02<00:02, 79.7MB/s]#015Downloading:  53%|█████▎    | 231M/436M [00:02<00:02, 79.7MB/s]#015Downloading:  55%|█████▍    | 239M/436M [00:03<00:02, 79.7MB/s]#015Downloading:  57%|█████▋    | 247M/436M [00:03<00:02, 79.7MB/s]#015Downloading:  58%|█████▊    | 255M/436M [00:03<00:02, 79.8MB/s]#015Downloading:  60%|██████    | 263M/436M [00:03<00:02, 79.8MB/s]#015Downloading:  62%|██████▏   | 271M/436M [00:03<00:02, 79.8MB/s]#015Downloading:  64%|██████▍   | 279M/436M [00:03<00:01, 79.9MB/s]#015Downloading:  66%|██████▌   | 287M/436M [00:03<00:01, 79.9MB/s]#015Downloading:  68%|██████▊   | 295M/436M [00:03<00:01, 79.9MB/s]#015Downloading:  69%|██████▉   | 303M/436M [00:03<00:01, 79.8MB/s]#015Downloading:  71%|███████▏  | 311M/436M [00:03<00:01, 79.8MB/s]#015Downloading:  73%|███████▎  | 319M/436M [00:04<00:01, 80.4MB/s]#015Downloading:  75%|███████▌  | 328M/436M [00:04<00:01, 83.4MB/s]#015Downloading:  77%|███████▋  | 337M/436M [00:04<00:01, 85.7MB/s]#015Downloading:  79%|███████▉  | 346M/436M [00:04<00:01, 87.3MB/s]#015Downloading:  82%|████████▏ | 356M/436M [00:04<00:00, 88.7MB/s]#015Downloading:  84%|████████▎ | 365M/436M [00:04<00:00, 89.6MB/s]#015Downloading:  86%|████████▌ | 374M/436M [00:04<00:00, 90.2MB/s]#015Downloading:  88%|████████▊ | 383M/436M [00:04<00:00, 90.6MB/s]#015Downloading:  90%|█████████ | 392M/436M [00:04<00:00, 91.1MB/s]#015Downloading:  92%|█████████▏| 402M/436M [00:04<00:00, 91.3MB/s]#015Downloading:  94%|█████████▍| 411M/436M [00:05<00:00, 91.4MB/s]#015Downloading:  96%|█████████▋| 420M/436M [00:05<00:00, 91.5MB/s]#015Downloading:  98%|█████████▊| 429M/436M [00:05<00:00, 91.6MB/s]#015Downloading: 100%|██████████| 436M/436M [00:05<00:00, 82.6MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [0/16909 (0%)]#011Loss: 0.497937\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3200/16909 (38%)]#011Loss: 0.000680\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/16909 (75%)]#011Loss: 0.000199\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Epochs: 1 | Train Loss:  0.000 | Train Accuracy:  0.995 | Val Loss:  0.000 | Val Accuracy:  1.000\u001b[0m\n",
      "\u001b[34m2022-10-10 06:59:18,714 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-10-10 06:59:41 Uploading - Uploading generated training model\n",
      "2022-10-10 07:00:41 Completed - Training job completed\n",
      "ProfilerReport-1665384354: IssuesFound\n",
      "Training seconds: 774\n",
      "Billable seconds: 774\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({\"train\": train_input}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca684446-fe9a-4935-a1dd-f9161af7b3e1",
   "metadata": {},
   "source": [
    "## Deploy a Hugging Face Transformer model from S3 to SageMaker for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ecf01-4ee3-4fd8-8d2e-d20e34dcad09",
   "metadata": {},
   "source": [
    "There are two ways on how you can deploy you SageMaker trained Hugging Face model from S3. \n",
    "1. Deploy the model directly after training\n",
    "\n",
    "```\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")\n",
    "\n",
    "# If you need to restart you can create the huggingface_estimator with a job name:\n",
    "from sagemaker.estimator import Estimator\n",
    "huggingface_estimator = Estimator.attach(\"huggingface-pytorch-training-xxxx\")\n",
    "\n",
    "```\n",
    "2. Deploy model later using the PytorchModel/HuggingFaceModel.\n",
    "\n",
    "```\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "import sagemaker \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "# zipped_model_path = 's3://bucket/model-artifacts/hf_model.tar.gz'\n",
    "\n",
    "model = PyTorchModel(entry_point='code/inference.py',\n",
    "                     name='hf-bert-model-train-deploy',\n",
    "                     model_data=zipped_model_path, \n",
    "                     role=role, \n",
    "                     framework_version='1.7.1', \n",
    "                     py_version='py3')\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.c4.8xlarge\",\n",
    "    endpoint_name='hf-bert-endpoint'\n",
    ")\n",
    "```\n",
    "\n",
    "Note: *The second method is more commonly used in practice.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc631e34-c0bb-4a4c-a0d3-2fa6459644b7",
   "metadata": {},
   "source": [
    "To demonstrate how we can use Sagemaker to deploy and batch transform, we will use the second approach that is 'deploy model later using the PytorchModel/HuggingFaceModel'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f6af40-e04f-4b2c-9b10-d69cb25f6273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-526539017075/sagemaker-script-mode-for-bert-2022-10-10-06-45-54-223/output/model.tar.gz to ./model.tar.gz\n",
      "model.bin\n"
     ]
    }
   ],
   "source": [
    "# download the model data for the trained model\n",
    "\n",
    "model_path = \"model/\" \n",
    "! aws s3 cp $huggingface_estimator.model_data model.tar.gz\n",
    "! mkdir -p $model_path\n",
    "! tar -xvf model.tar.gz -C  $model_path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d43eea-2cd3-4556-8686-1f7cf8abbc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress the scripts and the model_data into a gz file and upload to S3\n",
    "\n",
    "import os   \n",
    "bucket = '<your bucket>'\n",
    "prefix = 'data/Womens-Clothing-E-Commerce-Reviews'\n",
    "\n",
    "import tarfile\n",
    "with tarfile.open('hf_model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add(model_path, recursive=True)\n",
    "    archive.add('code/')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "zipped_model_path = sagemaker_session.upload_data(path='hf_model.tar.gz', bucket=bucket, key_prefix='model-artifacts') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39f267b7-66fb-41ba-9991-1a54e24e794d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: hf-bert-model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "# create Model\n",
    "\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "import sagemaker \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "model = PyTorchModel(entry_point='code/inference.py',\n",
    "                     name='hf-bert-model-train-deploy',\n",
    "                     model_data=zipped_model_path, \n",
    "                     role=role, \n",
    "                     framework_version='1.7.1', \n",
    "                     py_version='py3')\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.c4.8xlarge\",\n",
    "    endpoint_name='hf-bert-endpoint'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f694bc-74bd-4b9c-a57b-0ecabaa4f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference the sample data via endpoint\n",
    "\n",
    "import pandas as pd\n",
    "test_df = pd.read_csv('data/samples.csv')\n",
    "\n",
    "# example request\n",
    "data = str(test_df.sample()['review'].values[0])\n",
    "\n",
    "# predictor = sagemaker.predictor.Predictor(endpoint_name, sagemaker.Session())\n",
    "predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.CSVDeserializer()\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcf0a38b-f932-4c71-a451-ee85154213b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7691a6-9223-43a5-859f-108403e26f6e",
   "metadata": {},
   "source": [
    "## Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde46820-16fa-46a5-9dad-c37146af4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the batch transform\n",
    "\n",
    "# specify your s3uri below for input and output of transform job\n",
    "## TODO\n",
    "batch_transform_input_s3uri = 's3://bucket/data/Womens-Clothing-E-Commerce-Reviews/samples.csv'\n",
    "batch_transform_output_s3uri = 's3://bucket/data/Womens-Clothing-E-Commerce-Reviews/output'\n",
    "\n",
    "transformer = model.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type='ml.c4.8xlarge', # ml.c5.4xlarge\n",
    "    output_path=batch_transform_output_s3uri,\n",
    "    assemble_with=\"Line\", \n",
    "    accept=\"text/csv\",\n",
    "    strategy='MultiRecord',\n",
    "    max_payload=25,\n",
    "    max_concurrent_transforms=4\n",
    ")\n",
    "\n",
    "transformer.transform(\n",
    "    data=batch_transform_input_s3uri,\n",
    "    content_type=\"text/csv\", \n",
    "    split_type='Line'\n",
    ")\n",
    "\n",
    "transformer.wait()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
